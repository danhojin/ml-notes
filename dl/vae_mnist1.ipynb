{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\log p(x) &= \\log \\int_z dz q(z | x) \\frac{p(x|z) p(z)}{q(x|z)} \\\\\n",
    "&\\ge \\int_z dz q(z | x) \\log  \\frac{p(x|z) p(z)}{q(x|z)} \\\\\n",
    "&= \\mathbb{E}_{Z \\sim q(z|x)} [\\log p(x|Z)] - \\mathbf{D}_{KL} (q(Z|x) || p(Z))\n",
    "\\end{align}\n",
    "\n",
    "학습 데이타를 잘 반영한다는 것은 maximum likelihood 처리와 비슷하게 $\\prod_{x \\in D} p(x)$를 최대화 하는 확률밀도함수 p(x)를 찾는 것이다. $\\log$를 취하면, $\\mathbb{E}_{X \\sim D} \\log p(X)$로 쓸 수 있다. 우변의 첫 항은 decoder에서 두번 째 항은 encoder와 관련되어 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading the MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 784])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    partial(torch.reshape, shape=(-1,)),\n",
    "])\n",
    "\n",
    "# Training dataset\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        root='./data',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=img_transform),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True)\n",
    "# Test dataset\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST(\n",
    "        root='./data',\n",
    "        train=False,\n",
    "        transform=img_transform),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False)\n",
    "\n",
    "# sample plot\n",
    "for imgs, targets in train_loader:\n",
    "    print(imgs.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building a variational autoencoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (fc11): Linear(in_features=784, out_features=512, bias=True)\n",
       "  (mu): Linear(in_features=512, out_features=2, bias=True)\n",
       "  (log_var): Linear(in_features=512, out_features=2, bias=True)\n",
       "  (fc21): Linear(in_features=2, out_features=512, bias=True)\n",
       "  (fc22): Linear(in_features=512, out_features=784, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, nx, nh, nz):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.nz = nz\n",
    "        \n",
    "        # encoder\n",
    "        self.fc11 = nn.Linear(nx, nh)\n",
    "        self.mu = nn.Linear(nh, nz)\n",
    "        self.log_var = nn.Linear(nh, nz)\n",
    "        \n",
    "        # decoder\n",
    "        self.fc21 = nn.Linear(nz, nh)\n",
    "        self.fc22 = nn.Linear(nh, nx)\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        h = F.relu(self.fc11(x))\n",
    "        return self.mu(h), self.log_var(h)\n",
    "    \n",
    "    def decoder(self, z):\n",
    "        h = F.relu(self.fc21(z))\n",
    "        return F.sigmoid(self.fc22(h))\n",
    "    \n",
    "    def sample_z(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encoder(x)\n",
    "        z = self.sample_z(mu, log_var)\n",
    "        return self.decoder(z), mu, log_var\n",
    "    \n",
    "vae = VAE(nx=784, nh=512, nz=2)\n",
    "vae.to(device)\n",
    "vae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\mathcal{D}_{KL} [ \\mathcal{N} (\\mu_0, \\Sigma_0) || \\mathcal{N} (0, I)] = \\frac{1}{2} \\sum_k \\left( \\exp(\\Sigma(X)) + \\mu^2 (X) - 1 - \\Sigma(X)\\right)\n",
    "\\end{align}\n",
    "\n",
    "#### Non-negative Kullback-Leibler divergence\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{D}_{KL} &= \\mathbb{E}_{P} \\log \\frac{P}{Q} \\\\\n",
    "&= \\mathbb{E}_{P} \\left(- \\log \\frac{Q}{P} \\right) \\\\\n",
    "&\\stackrel{\\text{Jensen's inequility}}{\\ge} -log \\mathbb{E}_{P} \\frac{Q}{P} \\; \\because -\\log(x) \\; \\text{is a convex function}\\\\\n",
    "&= - \\log 1 \\\\\n",
    "&= 0\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(vae.parameters())\n",
    "\n",
    "def loss_function(recon_x, x, mu, log_var):\n",
    "    BCE = -F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    DKL = 0.5 * torch.sum(log_var.exp() + mu.pow(2) - 1 - log_var)\n",
    "    return -(BCE - DKL)  # first minus sign to maximize\n",
    "\n",
    "def train(epoch):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, log_var = vae(data)\n",
    "        loss = loss_function(recon_batch, data, mu, log_var)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item() / len(data)))\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "        epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danhojin/miniconda2/envs/ray2/lib/python3.7/site-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 548.268066\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 192.383881\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 176.883469\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 174.870499\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 168.433914\n",
      "====> Epoch: 0 Average loss: 186.3493\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 169.631699\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 164.975983\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 165.128540\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 159.299683\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 163.419098\n",
      "====> Epoch: 1 Average loss: 165.4888\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 167.990204\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 164.140518\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 163.831207\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 160.021576\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 156.648911\n",
      "====> Epoch: 2 Average loss: 161.6146\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 161.762924\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 156.636169\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 158.688568\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 152.112167\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 157.425842\n",
      "====> Epoch: 3 Average loss: 159.3400\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 163.783844\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 160.807968\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 151.812363\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 156.350586\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 158.543686\n",
      "====> Epoch: 4 Average loss: 157.5723\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "1. D. Carl, Tutorial on variational autoencoders, arXiv:1606.05908v2, 2016\n",
    "2. R.G. Krishnan, U. Shalit, D. Sontag, Deep Kalman Filters, arXiv:1511.05121v2, 2015\n",
    "3. J. Duchi, [Derivations for linear algebra and optimization](http://web.stanford.edu/~jduchi/projects/general_notes.pdf)\n",
    "4. A. Kristladl, [Variational autoencoder: intuition and implementation](https://wiseodd.github.io/techblog/2016/12/10/variational-autoencoder/), blog post 2016\n",
    "5. https://github.com/lyeoni/pytorch-mnist-VAE\n",
    "6. [mxnet variational autoencoder example](https://github.com/apache/incubator-mxnet/tree/master/example/autoencoder/variational_autoencoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
