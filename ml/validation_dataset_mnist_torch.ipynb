{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation dataset\n",
    "\n",
    "목적: overfitting을 방지하기 위한 방편으로 validation dataset을 사용하는 torch코드를 정리한다. (keras를 이용하면 더 손쉽게 동일 목적을 달성할 수 있다.)\n",
    "\n",
    "validation dataset 분리: torch.utils.data.random_split를 이용하여 train dataset의 일부를 validation dataset으로 떼어둔다.\n",
    "\n",
    "epochs: 충분히 긴 epochs를 이용하여 validation loss가 saturation되는 epochs를 확인하고, 이 값을 가지고 다시 학습을 진행한다.\n",
    "\n",
    "결과: 마지막으로 test datasets에서 결과를 검토하였다. validation loss와 비슷한 test loss를 확일할 수 있다. 단, 일반적으로 test datasets에 대한 결과를 알 수 없다는 점을 염두에 두어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([42000, 18000], 60000)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train = datasets.MNIST(\n",
    "    './data', train=True, download=True,\n",
    "    transform=img_transforms)\n",
    "\n",
    "validation_split = 0.3\n",
    "lengths = [int(len(train) * validation_split)]\n",
    "lengths.insert(0, len(train) - lengths[0])\n",
    "lengths, sum(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train, val = torch.utils.data.random_split(\n",
    "    train,\n",
    "    lengths=lengths\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val, batch_size=BATCH_SIZE, shuffle=False\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    datasets.MNIST(\n",
    "        './data', train=False, download=True,\n",
    "        transform=img_transforms),\n",
    "    batch_size=BATCH_SIZE, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model and evaluation from \n",
    "\n",
    "https://github.com/pytorch/examples/blob/master/mnist/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    correct /= len(test_loader.dataset)\n",
    "    \n",
    "    return test_loss, correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train\n",
    "\n",
    "the negative log-likelihood loss\n",
    "\n",
    "https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import altair as alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1, loss: 0.5842, val_loss: 0.1998, val_correct: 0.9416\n",
      "epoch:   2, loss: 0.2357, val_loss: 0.1252, val_correct: 0.9628\n",
      "epoch:   3, loss: 0.1651, val_loss: 0.0883, val_correct: 0.9740\n",
      "epoch:   4, loss: 0.1287, val_loss: 0.0708, val_correct: 0.9782\n",
      "epoch:   5, loss: 0.1081, val_loss: 0.0615, val_correct: 0.9809\n",
      "epoch:   6, loss: 0.0949, val_loss: 0.0564, val_correct: 0.9820\n",
      "epoch:   7, loss: 0.0864, val_loss: 0.0518, val_correct: 0.9841\n",
      "epoch:   8, loss: 0.0760, val_loss: 0.0488, val_correct: 0.9847\n",
      "epoch:   9, loss: 0.0708, val_loss: 0.0463, val_correct: 0.9858\n",
      "epoch:  10, loss: 0.0642, val_loss: 0.0458, val_correct: 0.9858\n",
      "epoch:  11, loss: 0.0578, val_loss: 0.0423, val_correct: 0.9868\n",
      "epoch:  12, loss: 0.0549, val_loss: 0.0412, val_correct: 0.9868\n",
      "epoch:  13, loss: 0.0518, val_loss: 0.0410, val_correct: 0.9868\n",
      "epoch:  14, loss: 0.0484, val_loss: 0.0399, val_correct: 0.9874\n",
      "epoch:  15, loss: 0.0438, val_loss: 0.0379, val_correct: 0.9879\n",
      "epoch:  16, loss: 0.0423, val_loss: 0.0374, val_correct: 0.9888\n",
      "epoch:  17, loss: 0.0421, val_loss: 0.0376, val_correct: 0.9889\n",
      "epoch:  18, loss: 0.0379, val_loss: 0.0367, val_correct: 0.9887\n",
      "epoch:  19, loss: 0.0370, val_loss: 0.0365, val_correct: 0.9884\n",
      "epoch:  20, loss: 0.0338, val_loss: 0.0349, val_correct: 0.9891\n",
      "epoch:  21, loss: 0.0334, val_loss: 0.0357, val_correct: 0.9881\n",
      "epoch:  22, loss: 0.0320, val_loss: 0.0356, val_correct: 0.9896\n",
      "epoch:  23, loss: 0.0283, val_loss: 0.0346, val_correct: 0.9896\n",
      "epoch:  24, loss: 0.0273, val_loss: 0.0354, val_correct: 0.9904\n",
      "epoch:  25, loss: 0.0282, val_loss: 0.0365, val_correct: 0.9892\n",
      "epoch:  26, loss: 0.0261, val_loss: 0.0371, val_correct: 0.9893\n",
      "epoch:  27, loss: 0.0235, val_loss: 0.0359, val_correct: 0.9893\n",
      "epoch:  28, loss: 0.0229, val_loss: 0.0369, val_correct: 0.9895\n",
      "epoch:  29, loss: 0.0225, val_loss: 0.0390, val_correct: 0.9891\n",
      "epoch:  30, loss: 0.0223, val_loss: 0.0356, val_correct: 0.9901\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "lr = 1e-4\n",
    "\n",
    "model = Net().to(device)\n",
    "criterion = nn.NLLLoss(reduction='sum')  # negative log likelihood loss\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=lr,\n",
    "    weight_decay=1e-5\n",
    ")\n",
    "\n",
    "history = dict(train=[], val=[])\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    val_loss, val_correct = test(model, device, val_loader)\n",
    "    history['train'].append(train_loss)\n",
    "    history['val'].append(val_loss)\n",
    "    \n",
    "    print(f'epoch: {epoch + 1:3d}, loss: {train_loss:.4f},'\n",
    "          f' val_loss: {val_loss:.4f}, val_correct: {val_correct:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "df = pd.concat([\n",
    "    pd.DataFrame({\n",
    "        'dataset': ['train'] * len(history['train']),\n",
    "        'loss': history['train']\n",
    "    }).reset_index(),\n",
    "    pd.DataFrame({\n",
    "        'dataset': ['val'] * len(history['train']),\n",
    "        'loss': history['val']\n",
    "    }).reset_index(),\n",
    "], ignore_index=True)\n",
    "\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-76e10b9df30147448393e69b66e73d7c\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    const outputDiv = document.getElementById(\"altair-viz-76e10b9df30147448393e69b66e73d7c\");\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.0.2?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-1716b58ad44b42f822ac4f0c01ba1f4a\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"dataset\"}, \"x\": {\"type\": \"quantitative\", \"field\": \"index\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"loss\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.0.2.json\", \"datasets\": {\"data-1716b58ad44b42f822ac4f0c01ba1f4a\": [{\"index\": 0, \"dataset\": \"train\", \"loss\": 0.5842016562847864}, {\"index\": 1, \"dataset\": \"train\", \"loss\": 0.23566479894093104}, {\"index\": 2, \"dataset\": \"train\", \"loss\": 0.16512991325060528}, {\"index\": 3, \"dataset\": \"train\", \"loss\": 0.12873120865367707}, {\"index\": 4, \"dataset\": \"train\", \"loss\": 0.1080721462340582}, {\"index\": 5, \"dataset\": \"train\", \"loss\": 0.09494795014744713}, {\"index\": 6, \"dataset\": \"train\", \"loss\": 0.08639409300259182}, {\"index\": 7, \"dataset\": \"train\", \"loss\": 0.07598777568907965}, {\"index\": 8, \"dataset\": \"train\", \"loss\": 0.07076516594773247}, {\"index\": 9, \"dataset\": \"train\", \"loss\": 0.06416242871398017}, {\"index\": 10, \"dataset\": \"train\", \"loss\": 0.05776002961113339}, {\"index\": 11, \"dataset\": \"train\", \"loss\": 0.054942200774238226}, {\"index\": 12, \"dataset\": \"train\", \"loss\": 0.05181692504315149}, {\"index\": 13, \"dataset\": \"train\", \"loss\": 0.04838834875538236}, {\"index\": 14, \"dataset\": \"train\", \"loss\": 0.04384311170237405}, {\"index\": 15, \"dataset\": \"train\", \"loss\": 0.042255536539213996}, {\"index\": 16, \"dataset\": \"train\", \"loss\": 0.04209592096010844}, {\"index\": 17, \"dataset\": \"train\", \"loss\": 0.03785272987683614}, {\"index\": 18, \"dataset\": \"train\", \"loss\": 0.036975018274216426}, {\"index\": 19, \"dataset\": \"train\", \"loss\": 0.03382964253425598}, {\"index\": 20, \"dataset\": \"train\", \"loss\": 0.03338800203232538}, {\"index\": 21, \"dataset\": \"train\", \"loss\": 0.03196721962520054}, {\"index\": 22, \"dataset\": \"train\", \"loss\": 0.028302800723484584}, {\"index\": 23, \"dataset\": \"train\", \"loss\": 0.02729735151642845}, {\"index\": 24, \"dataset\": \"train\", \"loss\": 0.02822604605129787}, {\"index\": 25, \"dataset\": \"train\", \"loss\": 0.0260638059150605}, {\"index\": 26, \"dataset\": \"train\", \"loss\": 0.023543712987786247}, {\"index\": 27, \"dataset\": \"train\", \"loss\": 0.022871717941193352}, {\"index\": 28, \"dataset\": \"train\", \"loss\": 0.022489942420096623}, {\"index\": 29, \"dataset\": \"train\", \"loss\": 0.022314380588985626}, {\"index\": 0, \"dataset\": \"val\", \"loss\": 0.19977828243043688}, {\"index\": 1, \"dataset\": \"val\", \"loss\": 0.12520941938294305}, {\"index\": 2, \"dataset\": \"val\", \"loss\": 0.08830816602706909}, {\"index\": 3, \"dataset\": \"val\", \"loss\": 0.0708006030983395}, {\"index\": 4, \"dataset\": \"val\", \"loss\": 0.06145257732603285}, {\"index\": 5, \"dataset\": \"val\", \"loss\": 0.056350421216752794}, {\"index\": 6, \"dataset\": \"val\", \"loss\": 0.05180142019854651}, {\"index\": 7, \"dataset\": \"val\", \"loss\": 0.04884539442592197}, {\"index\": 8, \"dataset\": \"val\", \"loss\": 0.04634134550889333}, {\"index\": 9, \"dataset\": \"val\", \"loss\": 0.04578972434997559}, {\"index\": 10, \"dataset\": \"val\", \"loss\": 0.042339759018686085}, {\"index\": 11, \"dataset\": \"val\", \"loss\": 0.04121388426091936}, {\"index\": 12, \"dataset\": \"val\", \"loss\": 0.041010321074061926}, {\"index\": 13, \"dataset\": \"val\", \"loss\": 0.03993490184677972}, {\"index\": 14, \"dataset\": \"val\", \"loss\": 0.03787368388970693}, {\"index\": 15, \"dataset\": \"val\", \"loss\": 0.0374419299893909}, {\"index\": 16, \"dataset\": \"val\", \"loss\": 0.03759390241569943}, {\"index\": 17, \"dataset\": \"val\", \"loss\": 0.036676433973842196}, {\"index\": 18, \"dataset\": \"val\", \"loss\": 0.036470305283864336}, {\"index\": 19, \"dataset\": \"val\", \"loss\": 0.0349311939213011}, {\"index\": 20, \"dataset\": \"val\", \"loss\": 0.03571746957302094}, {\"index\": 21, \"dataset\": \"val\", \"loss\": 0.0355795683728324}, {\"index\": 22, \"dataset\": \"val\", \"loss\": 0.03463938289218479}, {\"index\": 23, \"dataset\": \"val\", \"loss\": 0.035391781449317934}, {\"index\": 24, \"dataset\": \"val\", \"loss\": 0.036512135558658176}, {\"index\": 25, \"dataset\": \"val\", \"loss\": 0.03714717821280161}, {\"index\": 26, \"dataset\": \"val\", \"loss\": 0.035906069464153714}, {\"index\": 27, \"dataset\": \"val\", \"loss\": 0.03692807336648305}, {\"index\": 28, \"dataset\": \"val\", \"loss\": 0.03895743325021532}, {\"index\": 29, \"dataset\": \"val\", \"loss\": 0.03557355429066552}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chart = alt.Chart(df).mark_line().encode(\n",
    "    x='index',\n",
    "    y='loss',\n",
    "    color='dataset'\n",
    ")\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# github won't render altair chart, so I save and load the history image.\n",
    "chart.save('mnist_split_history.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![train loss history](mnist_split_history.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final retrain, epochs = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1, loss: 0.6252, val_loss: 0.2356, val_correct: 0.9292\n",
      "epoch:   2, loss: 0.2604, val_loss: 0.1410, val_correct: 0.9581\n",
      "epoch:   3, loss: 0.1830, val_loss: 0.1022, val_correct: 0.9685\n",
      "epoch:   4, loss: 0.1417, val_loss: 0.0765, val_correct: 0.9772\n",
      "epoch:   5, loss: 0.1167, val_loss: 0.0656, val_correct: 0.9794\n",
      "epoch:   6, loss: 0.1032, val_loss: 0.0600, val_correct: 0.9811\n",
      "epoch:   7, loss: 0.0892, val_loss: 0.0540, val_correct: 0.9836\n",
      "epoch:   8, loss: 0.0788, val_loss: 0.0499, val_correct: 0.9849\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 8\n",
    "lr = 1e-4\n",
    "\n",
    "model = Net().to(device)\n",
    "criterion = nn.NLLLoss(reduction='sum')  # negative log likelihood loss\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=lr,\n",
    "    weight_decay=1e-5\n",
    ")\n",
    "\n",
    "history = dict(train=[], val=[])\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    val_loss, val_correct = test(model, device, val_loader)\n",
    "    history['train'].append(train_loss)\n",
    "    history['val'].append(val_loss)\n",
    "    \n",
    "    print(f'epoch: {epoch + 1:3d}, loss: {train_loss:.4f},'\n",
    "          f' val_loss: {val_loss:.4f}, val_correct: {val_correct:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_loss: 0.04348286626338959, test_correct: 0.9847\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_correct = test(model, device, test_loader)\n",
    "print(f'test_loss: {test_loss}, test_correct: {test_correct}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
